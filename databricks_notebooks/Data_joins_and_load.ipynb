{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cbdfcb6-e581-48a2-81cd-6b9dec439d0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## NOTE, THIS NOTEBOOK RUNS ONLY ON DATABRICKS\n",
    "If run without Databricks on Spark, Spark context etc. should be set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ff82669-7c09-4788-a4e2-65b7d8346fbc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. First setup the connection to S3 for data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9db298a6-4358-4a66-b440-cc0f70417738",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "868ce97c-867d-4758-9ed1-a45aa041d849",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "access_key = dbutils.secrets.get(scope = \"s3-access\", key = \"aws-access-key\")\n",
    "secret_key = dbutils.secrets.get(scope = \"s3-access\", key = \"aws-secret-key\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "\n",
    "# If you are using Auto Loader file notification mode to load files, provide the AWS Region ID.\n",
    "aws_region = \"us-west-1\"\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.\" + aws_region + \".amazonaws.com\")\n",
    "\n",
    "s3_bucket = \"s3a://ejkquant-uswest1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "179f8912-7fee-4876-9f78-ed757ab58160",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Then we load the intermediate results (bucketed data) from DBFS, join them with Datetime, and fill null values. Finally, we store the compelete train data set back to S3 for model training in later stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59ba8dae-7169-4a97-99bd-22095d5180ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t1 = spark.table(\"MSFT\")\n",
    "t2 = spark.table(\"SP500\")\n",
    "t3 = spark.table(\"BOND\")\n",
    "t4 = spark.table(\"NOTE\")\n",
    "t5 = spark.table(\"GOLD\")\n",
    "t6 = spark.table(\"econ\")\n",
    "\n",
    "dataset = t1.join(t2, how=\"left\", on=\"Datetime\")\n",
    "dataset = dataset.join(t3, how=\"left\", on=\"Datetime\")\n",
    "dataset = dataset.join(t4, how=\"left\", on=\"Datetime\")\n",
    "dataset = dataset.join(t5, how=\"left\", on=\"Datetime\")\n",
    "dataset = dataset.join(t6, how=\"left\", on=\"Datetime\")\n",
    "\n",
    "#dataset = forwardFillImputer(dataset, dataset.columns[1:3])\n",
    "dataset = dataset.fillna(0)\n",
    "dataset = dataset.withColumn(\"year\", year(col(\"Datetime\")))\n",
    "\n",
    "dataset.write.format(\"parquet\").partitionBy(\"year\").mode(\"overwrite\").save(s3_bucket+\"train_data\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "ea6c8adc-5d3b-4804-a194-221a1ca1dcd2",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "15d5eb12-6b06-486e-affe-9c831b828c08",
     "origId": 26068002939719,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data_joins_and_load",
   "notebookOrigID": 1382935014564674,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
